In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method. It was first developed by Evelyn Fix and Joseph Hodges in 1951,[1] and later expanded by Thomas Cover.[2] Most often, it is used for classification, as a k-NN classifier, the output of which is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.

- If your have N items in your set, a good solution is consider sqrt(N) neighbors.
- The k-nearest neighbors algorithm is used for classification and also for regression. It involves looking at the K nearest neighbors.
- Classification = assigning to groups.
- Regression = predicting an output (such as a number).
- Feature extraction means converting an item (such as a fruit or a user) into a list of numbers that can be compared.
- Choosing good features is an important part of making a k-nearest neighbors algorithm work correctly.